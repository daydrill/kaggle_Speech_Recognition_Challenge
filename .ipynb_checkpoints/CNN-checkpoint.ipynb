{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "\n",
    "LABELS = ['_silence', '_unknown', 'down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']\n",
    "TRAIN_PATH = './input/train/audio/'\n",
    "OUTPUT_PATH = './output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## custom_fft and log_specgram functions written by DavidS.\n",
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT 는 대칭(simmetrical)이므로 반쪽만 얻음.\n",
    "    # FFT 는 복소수이므로 실수값만 취하기 위해 abs()\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## utility function to grab all wav files inside train data folder.\n",
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples, L=16000):\n",
    "    '''\n",
    "    pad audios that are less than 16000(1 second) with 0s to make them all have the same length.\n",
    "    '''\n",
    "    if len(samples) >= L: \n",
    "        return samples\n",
    "    else: \n",
    "        return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0)) \n",
    "        # sample 앞뒤로 constant_values[0]과 constant_values[1]을 각각 pad_width 갯수 만큼 패딩\n",
    "        # 총길이는 len(samples) + 2*pad_width\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    '''\n",
    "    chop audios that are larger than 16000(eg. wav files in background noises folder) to 16000 in length.\n",
    "    create several chunks out of one large wav files given the parameter 'num'.\n",
    "    '''\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    '''\n",
    "    레이블 정규화 및 one-hot벡터화 (더미화)\n",
    "    '''\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('_silence')\n",
    "        elif label not in LABELS:\n",
    "            nlabels.append('_unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(nlabels)\n",
    "    nlabels = encoder.transform(nlabels)\n",
    "    return nlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load labels, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/train/audio/\n"
     ]
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chad/anaconda3/lib/python3.6/site-packages/scipy/io/wavfile.py:221: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 31.3 s, total: 1min 37s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(TRAIN_PATH, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: \n",
    "        n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y.append(label)\n",
    "        X.append(specgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X = X.reshape(tuple(list(X.shape) + [1])) # (64841, 99, 81, 1) 로 reshape\n",
    "y = to_categorical(label_transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train Validation Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=1130) # 9:1로 train, valid 셋 나눔."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (99, 81, 1) # in order to fit into Conv2D layer, we need to reshape it.\n",
    "nclass = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 99, 81, 1)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 99, 81, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 98, 80, 8)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 97, 79, 8)         264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 48, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 48, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 46, 37, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 44, 35, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 20, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2240)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               1147392   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 1,487,744\n",
      "Trainable params: 1,486,718\n",
      "Non-trainable params: 1,026\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Modeling\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(512, activation=activations.relu)(img_1))\n",
    "dense_1 = Dropout(rate=0.2)(dense_1)\n",
    "dense_1 = Dense(512, activation=activations.relu)(dense_1)\n",
    "dense_1 = Dropout(rate=0.2)(dense_1)\n",
    "dense_1 = Dense(128, activation=activations.relu)(dense_1)\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 네트워크 시각화\n",
    "plot_model(model, to_file='output/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45388 samples, validate on 19453 samples\n",
      "Epoch 1/10\n",
      "45388/45388 [==============================] - 183s - loss: 1.4195 - acc: 0.6185 - val_loss: 2.2121 - val_acc: 0.5892\n",
      "Epoch 2/10\n",
      "45388/45388 [==============================] - 186s - loss: 0.9823 - acc: 0.6812 - val_loss: 1.7682 - val_acc: 0.7480\n",
      "Epoch 3/10\n",
      "45388/45388 [==============================] - 10909s - loss: 0.7293 - acc: 0.7564 - val_loss: 1.4457 - val_acc: 0.8037\n",
      "Epoch 4/10\n",
      "45388/45388 [==============================] - 7441s - loss: 0.5902 - acc: 0.8027 - val_loss: 1.2901 - val_acc: 0.8184\n",
      "Epoch 5/10\n",
      "45388/45388 [==============================] - 726s - loss: 0.4974 - acc: 0.8336 - val_loss: 0.9550 - val_acc: 0.8591\n",
      "Epoch 6/10\n",
      "45388/45388 [==============================] - 170s - loss: 0.4378 - acc: 0.8521 - val_loss: 0.7716 - val_acc: 0.8606\n",
      "Epoch 7/10\n",
      "45388/45388 [==============================] - 173s - loss: 0.3926 - acc: 0.8690 - val_loss: 0.6087 - val_acc: 0.8841\n",
      "Epoch 8/10\n",
      "45388/45388 [==============================] - 184s - loss: 0.3542 - acc: 0.8818 - val_loss: 0.4977 - val_acc: 0.8919\n",
      "Epoch 9/10\n",
      "45388/45388 [==============================] - 185s - loss: 0.3218 - acc: 0.8909 - val_loss: 0.4130 - val_acc: 0.8947\n",
      "Epoch 10/10\n",
      "45388/45388 [==============================] - 186s - loss: 0.2987 - acc: 0.9008 - val_loss: 0.3569 - val_acc: 0.9017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19bda0470>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=1024, validation_data=(X_valid, y_valid), epochs=10, shuffle=True, verbose=1,)\n",
    "# model.save(os.path.join(OUTPUT_PATH, 'cnn_model2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(OUTPUT_PATH, 'cnn_custom_epoch10.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19453/19453 [==============================] - 26s    \n"
     ]
    }
   ],
   "source": [
    "preds_proba = model.predict(X_valid, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 정확도 : 0.90166\n",
      "\n",
      " -------------------------- \n",
      "\n",
      "preds     _silence  _unknown  down   go  left   no  off   on  right  stop  \\\n",
      "actuals                                                                     \n",
      "_silence        12        23     0    0     0    0    0    0      0     0   \n",
      "_unknown         0     11922    32   28    16   16   12   45     96    58   \n",
      "down             1       120   512    7     0   23    0    1      0    17   \n",
      "go               0       144    30  412     0   65    0    0      1    11   \n",
      "left             0        92     0    0   547    0    1    0      8     0   \n",
      "no               0        97    24   18     0  553    3    0      0     6   \n",
      "off              0        53     0    1     0    0  547   11      0     2   \n",
      "on               0       127     0    1     0    0   17  544      0     1   \n",
      "right            0        67     0    0     4    0    0    2    615     0   \n",
      "stop             0        46     1    0     0    0    1    0      0   602   \n",
      "up               0        36     0    0     1    0    4    0      0     7   \n",
      "yes              0        72     1    0     6    2    0    0      0     0   \n",
      "\n",
      "preds      up  yes  \n",
      "actuals             \n",
      "_silence    1    0  \n",
      "_unknown  145   10  \n",
      "down        5    1  \n",
      "go         16    1  \n",
      "left       22   10  \n",
      "no          4    5  \n",
      "off       140    0  \n",
      "on         19    0  \n",
      "right       2    0  \n",
      "stop       72    0  \n",
      "up        666    0  \n",
      "yes         2  608  \n",
      "\n",
      " -------------------------- \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   _silence       0.92      0.33      0.49        36\n",
      "   _unknown       0.93      0.96      0.95     12380\n",
      "       down       0.85      0.75      0.80       687\n",
      "         go       0.88      0.61      0.72       680\n",
      "       left       0.95      0.80      0.87       680\n",
      "         no       0.84      0.78      0.81       710\n",
      "        off       0.94      0.73      0.82       754\n",
      "         on       0.90      0.77      0.83       709\n",
      "      right       0.85      0.89      0.87       690\n",
      "       stop       0.86      0.83      0.84       722\n",
      "         up       0.61      0.93      0.74       714\n",
      "        yes       0.96      0.88      0.92       691\n",
      "\n",
      "avg / total       0.91      0.90      0.90     19453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = [LABELS[i] for i in np.argmax(preds_proba, axis=1)]\n",
    "actuals = [LABELS[i] for i in np.argmax(y_valid, axis=1)]\n",
    "print('* 정확도 : %.5f' % (np.sum(np.array(actuals) == np.array(preds)) / float(len(actuals))))\n",
    "preds = pd.Categorical(preds, categories=LABELS)\n",
    "actuals = pd.Categorical(actuals, categories=LABELS)\n",
    "print('\\n -------------------------- \\n')\n",
    "print(pd.crosstab(actuals, preds, rownames=['actuals'], colnames=['preds']))\n",
    "print('\\n -------------------------- \\n')\n",
    "print(classification_report(actuals, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
